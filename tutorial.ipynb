{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "941d215b-8114-43e8-bc17-6bde27c169f2",
   "metadata": {},
   "source": [
    "# Code for bot analysis in _Handbook of Computational Social Science_\n",
    "\n",
    "#### _Simon Lindgren_\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0edc3-1475-4929-893a-a040daa14da9",
   "metadata": {},
   "source": [
    "Import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7498197-cfa4-4507-959e-5154ef9d412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "from pywaffle import Waffle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from pprint import pprint\n",
    "from gensim import corpora, models\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "import squarify  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc6baa-aa35-439f-b384-248395c2a4e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "#### 1. Example of bot dection scores\n",
    "\n",
    "Based on a dataset of #antifa tweets (2016-2020, n = ~5.7M), we have data from the Botometer API for the top 10k users based on posting volume.\n",
    "\n",
    "Parse these botscores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeac2f4-d833-4703-aa25-22713358a273",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "caps = []\n",
    "types = []\n",
    "userids = []\n",
    "\n",
    "with open('antifa-botscores.txt', 'r') as data:\n",
    "    for i in data:\n",
    "        try:\n",
    "            i = ast.literal_eval(i)[1]\n",
    "            cap = i['cap']['english']\n",
    "            caps.append(cap)\n",
    "            typedict = i['display_scores']['english']\n",
    "            bottype = max(typedict, key=typedict.get)\n",
    "            types.append(bottype)\n",
    "            userid = i['user']['user_data']['id_str']\n",
    "            userids.append(userid)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "df = pd.DataFrame(zip(userids,caps,types))\n",
    "df.columns = ['user_id', 'cap', 'type']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa726ffd-f803-458e-9884-ada73a334f6a",
   "metadata": {},
   "source": [
    "Visualise the share of bot accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437282ef-af22-4630-b02b-a8c0fd82600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "botshare = len(df.cap[df.cap > threshold]) / len(df.cap)\n",
    "\n",
    "deleted = round((10000 - len(df)) / 50) # some accounts were not found by Botometer\n",
    "bots = round((botshare * 10000) / 50)\n",
    "humans = round((10000/50) * (1-botshare))\n",
    "\n",
    "myfont1 = mpl.font_manager.FontProperties(fname=\"Roboto\")\n",
    "\n",
    "fig = plt.figure(\n",
    "    FigureClass=Waffle,\n",
    "    figsize = (7,20),\n",
    "    rows=10,\n",
    "    values=[humans, bots, deleted],\n",
    "    colors=[\"black\", \"hotpink\", \"lightgrey\"],\n",
    "    icons=['user', 'robot', 'circle'],\n",
    "    font_size=14,\n",
    "    icon_style='solid',\n",
    "    icon_legend=True,\n",
    "    legend={\n",
    "        'frameon' : False,\n",
    "        'labels': ['CAP < 0.8', 'CAP ≥ 0.8', 'Deleted'], \n",
    "        'loc': 'upper left', \n",
    "        'bbox_to_anchor': (1, 1),\n",
    "    }\n",
    ")\n",
    "\n",
    "plt.savefig('antifa.png', dpi=300)\n",
    "#plt.savefig('antifa.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13b5ceb-7dbb-44fb-8866-06730ff256e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "#### 2. SNA example\n",
    "\n",
    "We have a csv edgelist based on mentions between accounts in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8806ca-be01-4480-b3ee-589464dfbb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgesDF.to_csv('antifa.csv', index = False) # to Gephi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b774f3d8-4ca2-42ef-8802-2acfcc38bf15",
   "metadata": {},
   "source": [
    "The csv was exported and analysed in Gephi, as described in the chapter.\n",
    "\n",
    "After initial processing in Gephi, 2090 nodes remained in the visualisation.\n",
    "\n",
    "Read their data based on Botometer API, and prepare csv with 'images' column for import in Gephi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eb0aad-76bf-4a07-98c6-54037bd1fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "images = []\n",
    "\n",
    "with open(\"gephi-botscores.txt\", \"r\") as raw:\n",
    "    \n",
    "    bot_treshold = 0.8\n",
    "    userid = []\n",
    "    for i in raw:\n",
    "        i = ast.literal_eval(i)\n",
    "        ids.append(i[1]['user']['user_data']['id_str'])\n",
    "        cap = i[1]['cap']['english']\n",
    "        if cap >= 0.8:\n",
    "            images.append('bot.png')\n",
    "        else:\n",
    "            images.append('human.png')  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c738ea-f9cb-43c7-82c0-f89caf103721",
   "metadata": {},
   "outputs": [],
   "source": [
    "botsDF = pd.DataFrame(zip(ids, images))\n",
    "botsDF.columns = ['Id', 'image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c530683a-06c0-4133-8ea1-517c82876ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "botsDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3352a189-c874-4c8c-a08c-c31183343ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "botsDF.to_csv(\"node_image_attrib.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3409a85-cc76-4021-b49c-ceb805124c59",
   "metadata": {},
   "source": [
    "The _Image Preview_ plugin for Gephi enabled visualising the network with icons for nodes. Numbered boxes were added in separate image editing software.\n",
    "\n",
    "![alt text](network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0d40b7-08ea-4d25-8e52-2c51cd19a74d",
   "metadata": {},
   "source": [
    "----\n",
    "#### 3. LDA example\n",
    "\n",
    "Reading all non–retweets from the #antifa dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921375d-030a-4fd9-bf39-1a2791547b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = []\n",
    "tweets = []\n",
    "\n",
    "with open('/path-to-dataset/antifa.jsonl') as indata:\n",
    "    for i in indata:\n",
    "        i = json.loads(i)\n",
    "        for d in i['data']:            \n",
    "            if not d['text'].startswith('RT'):\n",
    "                users.append(d['author_id'])\n",
    "                tweets.append(d['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cdfbdd-0a45-40bb-915a-5ee836cc571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c884d65-4959-4fa6-87e4-d0ce259724b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaDF = pd.DataFrame(zip(users,tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954b5c48-9375-4c32-a737-1445172f9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaDF.columns=['user', 'tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c7502-adc1-4f9a-bf87-80b528a27ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = ldaDF.groupby('user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d88e8-073e-4fe0-a375-64497a7b4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "usrs = []\n",
    "twts = []\n",
    "\n",
    "for user, df_group in df_grouped:\n",
    "    usrs.append(user)\n",
    "    twt = []\n",
    "    for t in df_group.tweet:\n",
    "        for w in t:\n",
    "            twt.append(w)\n",
    "    twts.append(' '.join(list(set(twt))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e85482-97d2-474b-93f8-5bb93419f905",
   "metadata": {},
   "source": [
    "Preprocessing ahead of LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732df82a-8a11-42db-9e36-b1cec8cffe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = twts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29f572f-56aa-4ac7-9167-1c54017bfaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e551e99-bfb7-41be-98d0-7485fbb7e0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f2dd60-f15c-4c75-981c-c34d1452c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f4db8b-7db2-4067-86e2-b195325ae62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur in less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff8b902-b1f9-4596-99c8-82b0e11d4fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468b35f-dfba-4143-936d-79c8c5df09fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf1c679-6aa7-47d8-a4a3-c6a037645277",
   "metadata": {},
   "source": [
    "Train the LDA model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadee404-0e7f-4ecd-90c6-321cf43e7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters.\n",
    "num_topics = 20\n",
    "chunksize = 2000\n",
    "passes = 10\n",
    "iterations = 200\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000d0365-1ad2-407f-9a37-0b42e832f98c",
   "metadata": {},
   "source": [
    "After manually inspecting the topics using\n",
    "\n",
    "```\n",
    "top_topics = model.top_topics(corpus)\n",
    "\n",
    "for c,t in enumerate(top_topics):\n",
    "    print(str(c) + ' ============\\n')\n",
    "    for i in t[0]:\n",
    "        print(i[1])\n",
    "    print('\\n\\n')\n",
    "````\n",
    "\n",
    "\n",
    "it was found that topics 14 and 3 were representative of anti-antifa discourse and antifa discourse respectively.\n",
    "\n",
    "Tag users by their top topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b9fc05-f078-43ec-b2fb-775d7d6070db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_category = []\n",
    "antianti = [14]\n",
    "antifa = [3]\n",
    "\n",
    "# Get the top topic for each user\n",
    "for user in model[corpus]:\n",
    "    top_topic = max(dict(user), key=dict(user).get)\n",
    "    \n",
    "    if top_topic in antianti:\n",
    "        user_category.append('antiantifa')\n",
    "        \n",
    "    if top_topic in antifa:\n",
    "        user_category.append('antifa')\n",
    "    \n",
    "    else:\n",
    "        user_category.append('0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be28a4-2bc0-4e97-8268-9606460debd1",
   "metadata": {},
   "source": [
    "Add in bot detection data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2f2f17-62b0-4c7d-9885-09f6fb764a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "viewsDF = pd.DataFrame(zip(usrs,user_category))\n",
    "viewsDF.columns = ['user', 'pos']\n",
    "viewsDF = viewsDF[viewsDF.pos != '0']\n",
    "\n",
    "dfX = pd.merge(viewsDF,botsDF, on = 'user')\n",
    "\n",
    "species = []\n",
    "for i in dfX.image:\n",
    "    if i == 'human.png':\n",
    "        species.append('humanoid')\n",
    "    else:\n",
    "        species.append('bot')\n",
    "\n",
    "dfX['image'] = species\n",
    "\n",
    "dfX.columns = ['user', 'pos', 'species']\n",
    "\n",
    "dfX.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e617b07-a299-4941-8874-6f2da1397256",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX['kind'] = dfX.pos + '_' + dfX.species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185f96fd-f312-4639-b832-730230967a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX.kind.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f20fa43-d307-4593-80b1-3a6fa70725ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame\n",
    "df = pd.DataFrame({'size':[35,12,4], 'group':[\"anti-antifa humanoid\", \"antifa humanoid\", \"anti-antifa bot\"] })\n",
    "\n",
    "# Plot it\n",
    "squarify.plot(sizes=df['size'], label=df['group'], alpha=.8 )\n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "plt.savefig('botplot.pdf')  \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
